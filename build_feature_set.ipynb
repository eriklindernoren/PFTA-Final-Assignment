{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from terminaltables import AsciiTable\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading articles: 100%|██████████| 1000000/1000000 [00:53<00:00, 18830.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load as Pandas DF...\n",
      "+ Done.\n"
     ]
    }
   ],
   "source": [
    "article_db = None\n",
    "\n",
    "if os.path.exists('data/articles'):\n",
    "    # Load saved reduced article db\n",
    "    print (\"Loading Article DB...\")\n",
    "    reduced_article_db = pd.read_pickle('data/articles')\n",
    "    print (\"+ Done.\")\n",
    "else:\n",
    "    # Load full article DB\n",
    "    lines = []\n",
    "    with open('data/signalmedia-1m.jsonl', 'r') as f:\n",
    "        for json_article in tqdm(f.readlines(), desc='Loading articles'):\n",
    "            article = dict(json.loads(json_article))\n",
    "            lines.append(article)\n",
    "\n",
    "    print (\"Load as Pandas DF...\")\n",
    "    article_db = pd.DataFrame(lines)\n",
    "    print (\"+ Done.\")\n",
    "\n",
    "    # Remove rows that has NaN values\n",
    "    article_db.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      "+ content\n",
      "+ id\n",
      "+ media-type\n",
      "+ published\n",
      "+ source\n",
      "+ title\n"
     ]
    }
   ],
   "source": [
    "columns = article_db.columns if article_db is not None else reduced_article_db.columns\n",
    "# Show column names\n",
    "print (\"Columns:\")\n",
    "for col in columns:\n",
    "    print (\"+ %s\" % col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce and balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "| Media Type | Count  |\n",
      "+------------+--------+\n",
      "| News       | 734488 |\n",
      "| Blog       | 265512 |\n",
      "+------------+--------+\n",
      "\n",
      "Reduce and balance dataset...\n",
      "+ Done.\n",
      "\n",
      "+------------+-------+\n",
      "| Media Type | Count |\n",
      "+------------+-------+\n",
      "| News       | 10000 |\n",
      "| Blog       | 10000 |\n",
      "+------------+-------+\n"
     ]
    }
   ],
   "source": [
    "def print_media_occ(articles):\n",
    "    \"\"\"Count number of occurences of each class and print as a table\"\"\"\n",
    "    num_news = np.sum(articles['blog'] == 0)\n",
    "    num_blogs = np.sum(articles['blog'] == 1)\n",
    "    \n",
    "    table_data = [[\"Media Type\", \"Count\"]]\n",
    "    for media_type, count in [(\"News\", num_news), (\"Blog\", num_blogs)]:\n",
    "        table_data.append([media_type, count])\n",
    "    print (AsciiTable(table_data).table)\n",
    "    \n",
    "if article_db is not None:\n",
    "    # One-hot encoding of 'media-type': 'News' = 0, 'Blog = 1\n",
    "    article_db.loc[article_db['media-type'] == 'News', 'blog'] = 0\n",
    "    article_db.loc[article_db['media-type'] == 'Blog', 'blog'] = 1\n",
    "\n",
    "    print_media_occ(article_db)\n",
    "    \n",
    "    print (\"\\nReduce and balance dataset...\")\n",
    "    n_of_each = 10000\n",
    "    news_articles = article_db[article_db['blog'] == 0].iloc[:n_of_each]\n",
    "    blog_articles = article_db[article_db['blog'] == 1].iloc[:n_of_each]\n",
    "    reduced_article_db = pd.concat([news_articles, blog_articles])\n",
    "    print (\"+ Done.\\n\")\n",
    "    \n",
    "    print_media_occ(reduced_article_db)\n",
    "    \n",
    "    reduced_article_db.to_pickle(\"data/articles\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Lemmatizing (the proper way, accounting for different POS tags)\n",
    "def penn_to_wn(penn_tag):\n",
    "    \"\"\"\n",
    "    Returns the corresponding WordNet POS tag for a Penn TreeBank POS tag.\n",
    "    \"\"\"\n",
    "    if penn_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        wn_tag = wn.NOUN\n",
    "    elif penn_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        wn_tag = wn.VERB\n",
    "    elif penn_tag in ['RB', 'RBR', 'RBS']:\n",
    "        wn_tag = wn.ADV\n",
    "    elif penn_tag in ['JJ', 'JJR', 'JJS']:\n",
    "        wn_tag = wn.ADJ\n",
    "    else:\n",
    "        wn_tag = None\n",
    "    return wn_tag\n",
    "\n",
    "def get_lemmas(tokens):\n",
    "    \"\"\"Lemmatize the list of tokens\"\"\"\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmas = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        wn_tag = penn_to_wn(pos)\n",
    "        lemma = lmtzr.lemmatize(token) if not wn_tag else lmtzr.lemmatize(token, wn_tag)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def token_count_in_articles(articles):\n",
    "    \"\"\"\n",
    "    Return occurence count of tokens in all articles.\n",
    "\n",
    "    :param DataFrame articles: Collection of articles\n",
    "    :returns int: A count value\n",
    "    \"\"\"\n",
    "    total_count = defaultdict(int)\n",
    "    for i in tqdm(range(len(articles)), desc='Counting token occurences'):\n",
    "        text = articles.iloc[i]['content'].lower()\n",
    "        token_count = get_token_count(text)\n",
    "        for token, count in token_count.items():\n",
    "            total_count[token] += count\n",
    "    return total_count\n",
    "\n",
    "def get_token_count(text):\n",
    "    \"\"\"\n",
    "    Count occurences of tokens in text.\n",
    "\n",
    "    :param str text: Text in which method counts tokens\n",
    "    :returns dict: Dict of tokens and their count\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    token_counts = dict(Counter(tokens))\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display discrepancies of token occurences between distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting token occurences: 100%|██████████| 10000/10000 [00:52<00:00, 191.75it/s]\n",
      "Counting token occurences: 100%|██████████| 10000/10000 [00:44<00:00, 225.82it/s]\n",
      "Determine occurence diff.: 100%|██████████| 232763/232763 [00:00<00:00, 497422.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+\n",
      "| Tokens with largest occurence diff. between media types |\n",
      "+---------------------------------------------------------+\n",
      "- Positive values indicate a higher occurence in blogs than news\n",
      "+------+-----------+------------+---------------+------------+\n",
      "| Num. | Token     | Count Diff | Token         | Count Diff |\n",
      "+------+-----------+------------+---------------+------------+\n",
      "| 0    | i         | 17999      | with          | -872       |\n",
      "| 1    | you       | 16514      | customers     | -867       |\n",
      "| 2    | ’         | 14686      | told          | -866       |\n",
      "| 3    | :         | 10121      | group         | -863       |\n",
      "| 4    | !         | 8657       | our           | -862       |\n",
      "| 5    | it        | 8016       | rights        | -849       |\n",
      "| 6    | your      | 7418       | against       | -837       |\n",
      "| 7    | s         | 6927       | thursday      | -834       |\n",
      "| 8    | my        | 6512       | australia     | -824       |\n",
      "| 9    | ?         | 5927       | announced     | -823       |\n",
      "| 10   | this      | 5712       | securities    | -823       |\n",
      "| 11   | that      | 4307       | two           | -815       |\n",
      "| 12   | so        | 3959       | media         | -813       |\n",
      "| 13   | t         | 3632       | markets       | -809       |\n",
      "| 14   | if        | 3509       | average       | -809       |\n",
      "| 15   | is        | 3369       | sunday        | -798       |\n",
      "| 16   | can       | 3266       | according     | -787       |\n",
      "| 17   | what      | 3184       | source        | -786       |\n",
      "| 18   | me        | 3040       | investment    | -772       |\n",
      "| 19   | –         | 2823       | five          | -763       |\n",
      "| 20   | like      | 2635       | country       | -762       |\n",
      "| 21   | but       | 2566       | world         | -760       |\n",
      "| 22   | [         | 2526       | -h-           | -760       |\n",
      "| 23   | do        | 2444       | former        | -757       |\n",
      "| 24   | ]         | 2292       | during        | -757       |\n",
      "| 25   | )         | 2262       | united        | -750       |\n",
      "| 26   | here      | 2257       | across        | -749       |\n",
      "| 27   | not       | 2170       | leader        | -748       |\n",
      "| 28   | &         | 2147       | capital       | -737       |\n",
      "| 29   | how       | 2038       | had           | -733       |\n",
      "| 30   | get       | 1997       | people        | -732       |\n",
      "| 31   | all       | 1956       | news          | -719       |\n",
      "| 32   | just      | 1950       | key           | -717       |\n",
      "| 33   | |         | 1949       | she           | -711       |\n",
      "| 34   | we        | 1921       | support       | -711       |\n",
      "| 35   | ”         | 1877       | contact       | -706       |\n",
      "| 36   | one       | 1826       | city          | -701       |\n",
      "| 37   | see       | 1800       | provides      | -697       |\n",
      "| 38   | out       | 1766       | provide       | -694       |\n",
      "| 39   | (         | 1735       | court         | -693       |\n",
      "| 40   | or        | 1727       | august        | -691       |\n",
      "| 41   | post      | 1673       | patients      | -691       |\n",
      "| 42   | some      | 1616       | years         | -684       |\n",
      "| 43   | #         | 1603       | news.net      | -683       |\n",
      "| 44   | there     | 1546       | international | -680       |\n",
      "| 45   | when      | 1527       | systems       | -672       |\n",
      "| 46   | even      | 1524       | four          | -666       |\n",
      "| 47   | love      | 1519       | operations    | -666       |\n",
      "| 48   | know      | 1391       | added         | -644       |\n",
      "| 49   | time      | 1387       | nasdaq        | -640       |\n",
      "| 50   | them      | 1357       | reserved      | -634       |\n",
      "| 51   | up        | 1206       | executive     | -630       |\n",
      "| 52   | then      | 1185       | sales         | -630       |\n",
      "| 53   | good      | 1155       | local         | -628       |\n",
      "| 54   | no        | 1149       | while         | -627       |\n",
      "| 55   | don       | 1133       | expected      | -624       |\n",
      "| 56   | free      | 1124       | conference    | -622       |\n",
      "| 57   | god       | 1120       | increase      | -620       |\n",
      "| 58   | “         | 1101       | corporation   | -612       |\n",
      "| 59   | really    | 1097       | north         | -607       |\n",
      "| 60   | things    | 1085       | director      | -603       |\n",
      "| 61   | ~         | 1083       | india         | -602       |\n",
      "| 62   | am        | 1078       | firm          | -598       |\n",
      "| 63   | now       | 1057       | china         | -595       |\n",
      "| 64   | re        | 1051       | bank          | -592       |\n",
      "| 65   | little    | 1041       | six           | -591       |\n",
      "| 66   | >         | 1038       | net           | -587       |\n",
      "| 67   | make      | 1008       | performance   | -584       |\n",
      "| 68   | much      | 1006       | officer       | -582       |\n",
      "| 69   | want      | 962        | period        | -581       |\n",
      "| 70   | d         | 954        | clients       | -579       |\n",
      "| 71   | because   | 951        | p.m.          | -577       |\n",
      "| 72   | too       | 948        | security      | -571       |\n",
      "| 73   | m         | 948        | c             | -567       |\n",
      "| 74   | these     | 925        | other         | -566       |\n",
      "| 75   | only      | 914        | platform      | -566       |\n",
      "| 76   | 1         | 880        | prime         | -565       |\n",
      "| 77   | great     | 877        | community     | -560       |\n",
      "| 78   | ;         | 870        | include       | -558       |\n",
      "| 79   | why       | 853        | sydney        | -556       |\n",
      "| 80   | go        | 851        | press         | -555       |\n",
      "| 81   | look      | 849        | risks         | -555       |\n",
      "| 82   | blog      | 839        | county        | -552       |\n",
      "| 83   | day       | 801        | senior        | -552       |\n",
      "| 84   | <         | 793        | $             | -551       |\n",
      "| 85   | something | 774        | annual        | -551       |\n",
      "| 86   | back      | 771        | further       | -550       |\n",
      "| 87   | google    | 771        | future        | -547       |\n",
      "| 88   | find      | 768        | reported      | -545       |\n",
      "| 89   | read      | 764        | association   | -544       |\n",
      "| 90   | always    | 757        | such          | -543       |\n",
      "| 91   | let       | 756        | analysis      | -542       |\n",
      "| 92   | ll        | 740        | board         | -541       |\n",
      "| 93   | book      | 735        | total         | -538       |\n",
      "| 94   | ·         | 732        | investors     | -538       |\n",
      "| 95   | they      | 730        | revenue       | -536       |\n",
      "| 96   | ve        | 725        | under         | -530       |\n",
      "| 97   | 3         | 721        | statement     | -527       |\n",
      "| 98   | way       | 719        | ceo           | -526       |\n",
      "| 99   | use       | 714        | -h            | -525       |\n",
      "+------+-----------+------------+---------------+------------+\n"
     ]
    }
   ],
   "source": [
    "news_articles = reduced_article_db[reduced_article_db['blog'] == 0]\n",
    "blog_articles = reduced_article_db[reduced_article_db['blog'] == 1]\n",
    "  \n",
    "token_count_news = token_count_in_articles(news_articles)\n",
    "token_count_blogs = token_count_in_articles(blog_articles)\n",
    "\n",
    "# Get all tokens\n",
    "tokens = set(token_count_news.keys())\n",
    "tokens.update(list(token_count_blogs.keys()))\n",
    "\n",
    "# Get the token occurence difference between news and blog articles\n",
    "diff_count = defaultdict(int)\n",
    "for token in tqdm(tokens, desc='Determine occurence diff.'):\n",
    "    diff_count[token] += token_count_blogs.get(token, 0)\n",
    "    diff_count[token] -= token_count_news.get(token, 0)\n",
    "\n",
    "# Sort the tokens by the largest occurence difference\n",
    "sorted_diff_count = sorted(diff_count.items(), key=lambda x: x[1], reverse=True)\n",
    "first_hundred = sorted_diff_count[:100]\n",
    "last_hundred = sorted_diff_count[-100::-1]\n",
    "\n",
    "# Display the tokens with the largest difference \n",
    "# (idea is that the count of tokens with a large occurence diff will make good features)\n",
    "print (AsciiTable([['Tokens with largest occurence diff. between media types']]).table)\n",
    "print (\"- Positive values indicate a higher occurence in blogs than news\")\n",
    "table_data = [['Num.', 'Token', 'Count Diff', 'Token', 'Count Diff']]\n",
    "for i in range(len(first_hundred)):\n",
    "    f_token, f_diff = first_hundred[i]\n",
    "    l_token, l_diff = last_hundred[i]\n",
    "    table_data.append([i, f_token, f_diff, l_token, l_diff])\n",
    "print (AsciiTable(table_data).table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens whose occurence count will act as features\n",
    "# Selected based on table above\n",
    "tokens_to_check = ['i', \n",
    "                   'you', \n",
    "                   'me', \n",
    "                   'we', \n",
    "                   'my', \n",
    "                   'this', \n",
    "                   'that', \n",
    "                   'it', \n",
    "                   'like',\n",
    "                   '--', \n",
    "                   '-', \n",
    "                   \"''\", \n",
    "                   '%', \n",
    "                   'said',\n",
    "                   'told',\n",
    "                   '?', \n",
    "                   '!', \n",
    "                   '’', \n",
    "                   ':']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building feature set: 100%|██████████| 20000/20000 [04:17<00:00, 77.81it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "feature_set = []\n",
    "for i in tqdm(range(reduced_article_db.shape[0]), desc='Building feature set'):\n",
    "    article = reduced_article_db.iloc[i]\n",
    "    article_text = article['content']\n",
    "    \n",
    "    article_features = {}\n",
    "    article_features['article_length'] = len(article_text)\n",
    "    \n",
    "    # Get token count in the article\n",
    "    token_counts = get_token_count(article_text.lower())\n",
    "\n",
    "    # Token count features\n",
    "    for token in tokens_to_check:\n",
    "        article_features[token] = token_counts.get(token, 0)\n",
    "        \n",
    "    # Sentiment features\n",
    "    sentiment = sid.polarity_scores(article_text)\n",
    "    article_features['positivity'] = sentiment['pos']\n",
    "    article_features['negativity'] = sentiment['neg']\n",
    "    article_features['neutral'] = sentiment['neu']\n",
    "    article_features['compound'] = sentiment['compound']\n",
    "    \n",
    "    # Target value\n",
    "    article_features['blog'] = article['blog']\n",
    "    \n",
    "    feature_set.append(article_features)\n",
    "    \n",
    "dataset = pd.DataFrame(feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "dataset.to_pickle('data/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
