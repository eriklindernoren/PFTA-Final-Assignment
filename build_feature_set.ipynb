{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from terminaltables import AsciiTable\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Article DB...\n",
      "+ Done.\n"
     ]
    }
   ],
   "source": [
    "article_db = None\n",
    "\n",
    "if os.path.exists(\"data/articles\"):\n",
    "    # Load saved reduced article db\n",
    "    print (\"Loading Article DB...\")\n",
    "    reduced_article_db = pd.read_pickle(\"data/articles\")\n",
    "    print (\"+ Done.\")\n",
    "else:\n",
    "    # Load full article DB and load as Pandas DF\n",
    "    lines = []\n",
    "    with open('data/signalmedia-1m.jsonl', 'r') as f:\n",
    "        for json_article in tqdm(f.readlines(), desc=\"Loading articles\"):\n",
    "            article = dict(json.loads(json_article))\n",
    "            lines.append(article)\n",
    "\n",
    "    print (\"Load as Pandas DF...\")\n",
    "    article_db = pd.DataFrame(lines)\n",
    "    print (\"+ Done.\")\n",
    "\n",
    "    # Remove rows that has NaN values\n",
    "    article_db.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      "+ content\n",
      "+ id\n",
      "+ media-type\n",
      "+ published\n",
      "+ source\n",
      "+ title\n",
      "+ blog\n"
     ]
    }
   ],
   "source": [
    "columns = article_db.columns if article_db is not None else reduced_article_db.columns\n",
    "# Show column names\n",
    "print (\"Columns:\")\n",
    "for col in columns:\n",
    "    print (\"+ %s\" % col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce and balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_media_occ(articles):\n",
    "    \"\"\"Count number of occurences of each class and print as a table\"\"\"\n",
    "    num_news = np.sum(articles['blog'] == 0)\n",
    "    num_blogs = np.sum(articles['blog'] == 1)\n",
    "    \n",
    "    table_data = [[\"Media Type\", \"Count\"]]\n",
    "    for media_type, count in [(\"News\", num_news), (\"Blog\", num_blogs)]:\n",
    "        table_data.append([media_type, count])\n",
    "    print (AsciiTable(table_data).table)\n",
    "    \n",
    "if article_db is not None:\n",
    "    # One-hot encoding of 'media-type': 'News' = 0, 'Blog = 1\n",
    "    article_db.loc[article_db['media-type'] == 'News', 'blog'] = 0\n",
    "    article_db.loc[article_db['media-type'] == 'Blog', 'blog'] = 1\n",
    "\n",
    "    print_media_occ(article_db)\n",
    "    \n",
    "    print (\"\\nReduce and balance dataset...\")\n",
    "    n_of_each = 3000\n",
    "    # Pick n_of_each articles of each media-type\n",
    "    news_articles = article_db[article_db['blog'] == 0].iloc[:n_of_each]\n",
    "    blog_articles = article_db[article_db['blog'] == 1].iloc[:n_of_each]\n",
    "    # Save as reduced article dataset\n",
    "    reduced_article_db = pd.concat([news_articles, blog_articles])\n",
    "    print (\"+ Done.\\n\")\n",
    "    \n",
    "    print_media_occ(reduced_article_db)\n",
    "    \n",
    "    reduced_article_db.to_pickle(\"data/articles\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "# Lemmatizing (the proper way, accounting for different POS tags)\n",
    "def penn_to_wn(penn_tag):\n",
    "    \"\"\"\n",
    "    Returns the corresponding WordNet POS tag for a Penn TreeBank POS tag.\n",
    "    \"\"\"\n",
    "    if penn_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        wn_tag = wn.NOUN\n",
    "    elif penn_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        wn_tag = wn.VERB\n",
    "    elif penn_tag in ['RB', 'RBR', 'RBS']:\n",
    "        wn_tag = wn.ADV\n",
    "    elif penn_tag in ['JJ', 'JJR', 'JJS']:\n",
    "        wn_tag = wn.ADJ\n",
    "    else:\n",
    "        wn_tag = None\n",
    "    return wn_tag\n",
    "\n",
    "def get_lemmas(tokens):\n",
    "    \"\"\"Lemmatize the list of tokens\"\"\"\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmas = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        wn_tag = penn_to_wn(pos)\n",
    "        lemma = lmtzr.lemmatize(token) if not wn_tag else lmtzr.lemmatize(token, wn_tag)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def get_token_count(articles):\n",
    "    \"\"\"\n",
    "    Return a collective count for the tokens in the articles.\n",
    "\n",
    "    :param articles: Pandas DF of articles\n",
    "    :returns: A count value\n",
    "    \"\"\"\n",
    "    total_count = defaultdict(int)\n",
    "    for i in tqdm(range(len(articles)), desc=\"Counting token occurences\"):\n",
    "        text = articles.iloc[i]['content']\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        token_count = dict(Counter(tokens))\n",
    "        for token, count in token_count.items():\n",
    "            total_count[token] += count\n",
    "    return total_count\n",
    "\n",
    "def sum_token_count(token_counts, tokens_to_check):\n",
    "    \"\"\"\n",
    "    Return the sum of counts for tokens in tokens_to_check.\n",
    "\n",
    "    :param token_counts: Dictionary with tokens and counts\n",
    "    :param tokens_to_check: List of tokens\n",
    "    :returns: A count value\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for token in tokens_to_check:\n",
    "        cnt += token_counts.get(token, 0)\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display discrepancies of token occurences between distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting token occurences: 100%|██████████| 3000/3000 [00:16<00:00, 180.62it/s]\n",
      "Counting token occurences: 100%|██████████| 3000/3000 [00:13<00:00, 220.07it/s]\n",
      "Determine occurence diff.: 100%|██████████| 128241/128241 [00:00<00:00, 466269.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------------+-------------+------------+\n",
      "| Num. | Token     | Count Diff | Token       | Count Diff |\n",
      "+------+-----------+------------+-------------+------------+\n",
      "| 0    | I         | 5143       | 2014        | -261       |\n",
      "| 1    | ’         | 4817       | Australia   | -259       |\n",
      "| 2    | you       | 4630       | customers   | -255       |\n",
      "| 3    | :         | 3345       | stock       | -254       |\n",
      "| 4    | !         | 2367       | August      | -249       |\n",
      "| 5    | s         | 2342       | across      | -248       |\n",
      "| 6    | your      | 2268       | which       | -245       |\n",
      "| 7    | it        | 2211       | management  | -241       |\n",
      "| 8    | ?         | 1799       | b           | -241       |\n",
      "| 9    | my        | 1718       | she         | -239       |\n",
      "| 10   | this      | 1435       | says        | -237       |\n",
      "| 11   | that      | 1430       | Thursday    | -236       |\n",
      "| 12   | is        | 1114       | Sunday      | -235       |\n",
      "| 13   | can       | 1091       | told        | -234       |\n",
      "| 14   | t         | 1041       | County      | -233       |\n",
      "| 15   | so        | 922        | over        | -233       |\n",
      "| 16   | me        | 914        | rating      | -233       |\n",
      "| 17   | –         | 872        | U.S.        | -232       |\n",
      "| 18   | ”         | 851        | with        | -229       |\n",
      "| 19   | You       | 802        | had         | -223       |\n",
      "| 20   | like      | 752        | average     | -222       |\n",
      "| 21   | we        | 718        | against     | -221       |\n",
      "| 22   | not       | 715        | SOURCE      | -219       |\n",
      "| 23   | what      | 682        | patients    | -219       |\n",
      "| 24   | but       | 663        | second      | -218       |\n",
      "| 25   | “         | 643        | country     | -215       |\n",
      "| 26   | all       | 640        | p.m.        | -209       |\n",
      "| 27   | It        | 638        | l-          | -209       |\n",
      "| 28   | if        | 632        | -r-ff-c     | -209       |\n",
      "| 29   | )         | 630        | provides    | -208       |\n",
      "| 30   | do        | 620        | during      | -206       |\n",
      "| 31   | or        | 616        | leader      | -206       |\n",
      "| 32   | This      | 605        | two         | -205       |\n",
      "| 33   | [         | 597        | future      | -204       |\n",
      "| 34   | out       | 553        | price       | -203       |\n",
      "| 35   | get       | 523        | India       | -202       |\n",
      "| 36   | some      | 513        | treatment   | -199       |\n",
      "| 37   | |         | 510        | expected    | -198       |\n",
      "| 38   | them      | 505        | win         | -197       |\n",
      "| 39   | just      | 500        | Saturday    | -197       |\n",
      "| 40   | And       | 499        | five        | -194       |\n",
      "| 41   | (         | 491        | added       | -194       |\n",
      "| 42   | If        | 487        | clients     | -192       |\n",
      "| 43   | God       | 477        | performance | -192       |\n",
      "| 44   | &         | 476        | j           | -191       |\n",
      "| 45   | <         | 463        | four        | -191       |\n",
      "| 46   | about     | 461        | Association | -190       |\n",
      "| 47   | ]         | 452        | online      | -189       |\n",
      "| 48   | know      | 452        | -r          | -189       |\n",
      "| 49   | ...       | 446        | markets     | -188       |\n",
      "| 50   | even      | 418        | Cup         | -187       |\n",
      "| 51   | >         | 418        | years       | -185       |\n",
      "| 52   | they      | 414        | period      | -184       |\n",
      "| 53   | one       | 411        | Group       | -184       |\n",
      "| 54   | us        | 404        | President   | -184       |\n",
      "| 55   | time      | 399        | CEO         | -183       |\n",
      "| 56   | post      | 381        | local       | -181       |\n",
      "| 57   | here      | 377        | South       | -178       |\n",
      "| 58   | there     | 375        | city        | -178       |\n",
      "| 59   | What      | 375        | US          | -175       |\n",
      "| 60   | see       | 372        | six         | -172       |\n",
      "| 61   | /         | 368        | -ch         | -171       |\n",
      "| 62   | how       | 355        | -f          | -171       |\n",
      "| 63   | re        | 345        | according   | -171       |\n",
      "| 64   | am        | 340        | mobile      | -171       |\n",
      "| 65   | #         | 328        | risks       | -169       |\n",
      "| 66   | good      | 326        | provide     | -169       |\n",
      "| 67   | So        | 324        | court       | -167       |\n",
      "| 68   | things    | 322        | United      | -167       |\n",
      "| 69   | love      | 322        | view        | -167       |\n",
      "| 70   | way       | 309        | chief       | -166       |\n",
      "| 71   | these     | 309        | firm        | -166       |\n",
      "| 72   | don       | 309        | operations  | -165       |\n",
      "| 73   | make      | 307        | economic    | -164       |\n",
      "| 74   | are       | 305        | former      | -164       |\n",
      "| 75   | up        | 292        | 're         | -163       |\n",
      "| 76   | m         | 289        | include     | -163       |\n",
      "| 77   | little    | 287        | Prime       | -162       |\n",
      "| 78   | much      | 282        | tax         | -162       |\n",
      "| 79   | My        | 272        | Market      | -162       |\n",
      "| 80   | then      | 272        | meeting     | -161       |\n",
      "| 81   | want      | 272        | state       | -161       |\n",
      "| 82   | Here      | 272        | lead        | -160       |\n",
      "| 83   | ve        | 269        | platform    | -160       |\n",
      "| 84   | really    | 265        | h           | -160       |\n",
      "| 85   | look      | 264        | Turnbull    | -160       |\n",
      "| 86   | only      | 264        | events      | -160       |\n",
      "| 87   | ll        | 259        | Total       | -160       |\n",
      "| 88   | too       | 257        | Reserved    | -159       |\n",
      "| 89   | be        | 251        | revenue     | -158       |\n",
      "| 90   | something | 240        | Bank        | -158       |\n",
      "| 91   | ;         | 239        | England     | -157       |\n",
      "| 92   | always    | 239        | economy     | -157       |\n",
      "| 93   | Apple     | 236        | media       | -157       |\n",
      "| 94   | When      | 232        | care        | -156       |\n",
      "| 95   | when      | 231        | players     | -156       |\n",
      "| 96   | Now       | 228        | June        | -155       |\n",
      "| 97   | right     | 226        | Rights      | -154       |\n",
      "| 98   | Is        | 224        | Table       | -153       |\n",
      "| 99   | More      | 219        | b-          | -152       |\n",
      "+------+-----------+------------+-------------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "news_articles = reduced_article_db[reduced_article_db['blog'] == 0]\n",
    "blog_articles = reduced_article_db[reduced_article_db['blog'] == 1]\n",
    "  \n",
    "token_count_news = get_token_count(news_articles)\n",
    "token_count_blogs = get_token_count(blog_articles)\n",
    "\n",
    "# Get all tokens\n",
    "tokens = list(token_count_news.keys())\n",
    "tokens.extend(list(token_count_blogs.keys()))\n",
    "tokens = set(tokens)\n",
    "\n",
    "# Get the token occurence difference between news and blog articles\n",
    "diff_count = defaultdict(int)\n",
    "for token in tqdm(tokens, desc=\"Determine occurence diff.\"):\n",
    "    diff_count[token] += token_count_blogs.get(token, 0)\n",
    "    diff_count[token] -= token_count_news.get(token, 0)\n",
    "\n",
    "# Sort the tokens by the largest occurence diff. and display the 200 where\n",
    "# the difference was the largest\n",
    "sorted_diff_count = sorted(diff_count.items(), key=lambda x: x[1], reverse=True)\n",
    "first_hundred = sorted_diff_count[:100]\n",
    "last_hundred = sorted_diff_count[-100::-1]\n",
    "\n",
    "# Display the tokens with the largest difference \n",
    "# (idea is that the count of tokens with a large occurence diff will make good features)\n",
    "table_data = [[\"Num.\", \"Token\", \"Count Diff\", \"Token\", \"Count Diff\"]]\n",
    "for i, ((f_token, f_diff), (l_token, l_diff)) in enumerate(zip(first_hundred, last_hundred)):\n",
    "    table_data.append([i, f_token, f_diff, l_token, l_diff])\n",
    "print (AsciiTable(table_data).table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building feature set: 6001it [02:12, 45.21it/s]                          \n"
     ]
    }
   ],
   "source": [
    "\n",
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def build_feature_set(article):\n",
    "    \"\"\"Turn article into feature set on which the model can train on.\"\"\"\n",
    "    article_text = article[\"content\"]\n",
    "\n",
    "    sample = pd.DataFrame()    \n",
    "    \n",
    "    # Length features\n",
    "    sample.loc[0, \"article_length\"] = len(article_text)\n",
    "    \n",
    "    # Get token count in the article\n",
    "    tokens = nltk.word_tokenize(article_text.lower())\n",
    "    token_counts = dict(Counter(tokens))\n",
    "    \n",
    "    # A list of tokens to check the occurence of in the article\n",
    "    tokens_to_check = [\n",
    "        (\"blog_tokens\", {\"i\", \"you\", \"me\", \"we\", \"my\", \"mine\", \"this\", \"that\", \"it\", \"is\", \"like\"}),\n",
    "        (\"news_char_occ\", {\"--\", \"-\", \"``\", \",\", \"''\", \";\", \"%\", \"said\"}),\n",
    "        (\"blog_char_occ\", {\"?\", \"!\", \"’\", \":\"}),\n",
    "    ]\n",
    "\n",
    "    # Sets of special tokens - Chosen by the token count for each article type above\n",
    "    for label, token_set in tokens_to_check:\n",
    "        # Get occurence in this article\n",
    "        occurence = sum_token_count(token_counts, token_set)\n",
    "        sample.loc[0, label] = occurence\n",
    "    \n",
    "    # Sentiment features\n",
    "    sentiment = sid.polarity_scores(article_text)\n",
    "    sample.loc[0, \"positivity\"] = sentiment['pos']\n",
    "    sample.loc[0, \"negativity\"] = sentiment['neg']\n",
    "    sample.loc[0, \"neutral\"] = sentiment['neu']\n",
    "    sample.loc[0, \"compound\"] = sentiment['compound']\n",
    "    \n",
    "    # Target value\n",
    "    sample.loc[0, \"blog\"] = article[\"blog\"]\n",
    "    \n",
    "    return sample.loc[0]\n",
    "\n",
    "\n",
    "# Progress bar for progress_apply\n",
    "tqdm.pandas(desc=\"Building feature set\")\n",
    "\n",
    "# For each article => extract feature set\n",
    "dataset = reduced_article_db.progress_apply(lambda x: build_feature_set(x), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "dataset.to_pickle(\"data/dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
