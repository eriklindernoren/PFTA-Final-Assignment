{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from terminaltables import AsciiTable\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading articles: 100%|██████████| 1000000/1000000 [00:52<00:00, 19072.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load as Pandas DF...\n",
      "+ Done.\n"
     ]
    }
   ],
   "source": [
    "article_db = None\n",
    "\n",
    "if os.path.exists(\"data/articles\"):\n",
    "    # Load saved reduced article db\n",
    "    print (\"Loading Article DB...\")\n",
    "    reduced_article_db = pd.read_pickle(\"data/articles\")\n",
    "    print (\"+ Done.\")\n",
    "else:\n",
    "    # Load full article DB and load as Pandas DF\n",
    "    lines = []\n",
    "    with open('data/signalmedia-1m.jsonl', 'r') as f:\n",
    "        for json_article in tqdm(f.readlines(), desc=\"Loading articles\"):\n",
    "            article = dict(json.loads(json_article))\n",
    "            lines.append(article)\n",
    "\n",
    "    print (\"Load as Pandas DF...\")\n",
    "    article_db = pd.DataFrame(lines)\n",
    "    print (\"+ Done.\")\n",
    "\n",
    "    # Remove rows that has NaN values\n",
    "    article_db.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      "+ content\n",
      "+ id\n",
      "+ media-type\n",
      "+ published\n",
      "+ source\n",
      "+ title\n"
     ]
    }
   ],
   "source": [
    "columns = article_db.columns if article_db is not None else reduced_article_db.columns\n",
    "# Show column names\n",
    "print (\"Columns:\")\n",
    "for col in columns:\n",
    "    print (\"+ %s\" % col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce and balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "| Media Type | Count  |\n",
      "+------------+--------+\n",
      "| News       | 734488 |\n",
      "| Blog       | 265512 |\n",
      "+------------+--------+\n",
      "\n",
      "Reduce and balance dataset...\n",
      "+ Done.\n",
      "\n",
      "+------------+-------+\n",
      "| Media Type | Count |\n",
      "+------------+-------+\n",
      "| News       | 5000  |\n",
      "| Blog       | 5000  |\n",
      "+------------+-------+\n"
     ]
    }
   ],
   "source": [
    "def print_media_occ(articles):\n",
    "    \"\"\"Count number of occurences of each class and print as a table\"\"\"\n",
    "    num_news = np.sum(articles['blog'] == 0)\n",
    "    num_blogs = np.sum(articles['blog'] == 1)\n",
    "    \n",
    "    table_data = [[\"Media Type\", \"Count\"]]\n",
    "    for media_type, count in [(\"News\", num_news), (\"Blog\", num_blogs)]:\n",
    "        table_data.append([media_type, count])\n",
    "    print (AsciiTable(table_data).table)\n",
    "    \n",
    "if article_db is not None:\n",
    "    # One-hot encoding of 'media-type': 'News' = 0, 'Blog = 1\n",
    "    article_db.loc[article_db['media-type'] == 'News', 'blog'] = 0\n",
    "    article_db.loc[article_db['media-type'] == 'Blog', 'blog'] = 1\n",
    "\n",
    "    print_media_occ(article_db)\n",
    "    \n",
    "    print (\"\\nReduce and balance dataset...\")\n",
    "    n_of_each = 5000\n",
    "    # Pick n_of_each articles of each media-type\n",
    "    news_articles = article_db[article_db['blog'] == 0].iloc[:n_of_each]\n",
    "    blog_articles = article_db[article_db['blog'] == 1].iloc[:n_of_each]\n",
    "    # Save as reduced article dataset\n",
    "    reduced_article_db = pd.concat([news_articles, blog_articles])\n",
    "    print (\"+ Done.\\n\")\n",
    "    \n",
    "    print_media_occ(reduced_article_db)\n",
    "    \n",
    "    reduced_article_db.to_pickle(\"data/articles\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "# Lemmatizing (the proper way, accounting for different POS tags)\n",
    "def penn_to_wn(penn_tag):\n",
    "    \"\"\"\n",
    "    Returns the corresponding WordNet POS tag for a Penn TreeBank POS tag.\n",
    "    \"\"\"\n",
    "    if penn_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        wn_tag = wn.NOUN\n",
    "    elif penn_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        wn_tag = wn.VERB\n",
    "    elif penn_tag in ['RB', 'RBR', 'RBS']:\n",
    "        wn_tag = wn.ADV\n",
    "    elif penn_tag in ['JJ', 'JJR', 'JJS']:\n",
    "        wn_tag = wn.ADJ\n",
    "    else:\n",
    "        wn_tag = None\n",
    "    return wn_tag\n",
    "\n",
    "def get_lemmas(tokens):\n",
    "    \"\"\"Lemmatize the list of tokens\"\"\"\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmas = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        wn_tag = penn_to_wn(pos)\n",
    "        lemma = lmtzr.lemmatize(token) if not wn_tag else lmtzr.lemmatize(token, wn_tag)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def get_token_count(articles):\n",
    "    \"\"\"\n",
    "    Return a collective count for the tokens in the articles.\n",
    "\n",
    "    :param DataFrame articles: Collection of articles\n",
    "    :returns int: A count value\n",
    "    \"\"\"\n",
    "    total_count = defaultdict(int)\n",
    "    for i in tqdm(range(len(articles)), desc=\"Counting token occurences\"):\n",
    "        text = articles.iloc[i]['content'].lower()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        token_count = dict(Counter(tokens))\n",
    "        for token, count in token_count.items():\n",
    "            total_count[token] += count\n",
    "    return total_count\n",
    "\n",
    "def sum_token_count(token_counts, tokens_to_check):\n",
    "    \"\"\"\n",
    "    For the list tokens_to_check, return the collective count of those tokens\n",
    "    according to token_counts.\n",
    "\n",
    "    :param list token_counts: Dictionary with tokens and counts\n",
    "    :param list tokens_to_check: List of tokens\n",
    "    :returns int: A count value\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for token in tokens_to_check:\n",
    "        cnt += token_counts.get(token, 0)\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display discrepancies of token occurences between distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting token occurences: 100%|██████████| 5000/5000 [00:25<00:00, 196.52it/s]\n",
      "Counting token occurences: 100%|██████████| 5000/5000 [00:23<00:00, 213.98it/s]\n",
      "Determine occurence diff.: 100%|██████████| 149710/149710 [00:00<00:00, 450530.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------------+---------------+------------+\n",
      "| Num. | Token     | Count Diff | Token         | Count Diff |\n",
      "+------+-----------+------------+---------------+------------+\n",
      "| 0    | i         | 8628       | results       | -464       |\n",
      "| 1    | you       | 8619       | thursday      | -462       |\n",
      "| 2    | ’         | 6995       | cent          | -458       |\n",
      "| 3    | :         | 4881       | country       | -450       |\n",
      "| 4    | !         | 4119       | rating        | -446       |\n",
      "| 5    | it        | 4100       | billion       | -444       |\n",
      "| 6    | your      | 4002       | average       | -439       |\n",
      "| 7    | s         | 3305       | rights        | -438       |\n",
      "| 8    | my        | 3299       | u.s.          | -438       |\n",
      "| 9    | this      | 2874       | r             | -434       |\n",
      "| 10   | ?         | 2864       | told          | -433       |\n",
      "| 11   | that      | 2307       | against       | -432       |\n",
      "| 12   | so        | 1953       | saturday      | -429       |\n",
      "| 13   | can       | 1839       | markets       | -427       |\n",
      "| 14   | if        | 1793       | court         | -427       |\n",
      "| 15   | is        | 1789       | government    | -424       |\n",
      "| 16   | t         | 1692       | sales         | -422       |\n",
      "| 17   | what      | 1622       | announced     | -422       |\n",
      "| 18   | me        | 1573       | city          | -417       |\n",
      "| 19   | –         | 1496       | customers     | -411       |\n",
      "| 20   | like      | 1334       | international | -409       |\n",
      "| 21   | do        | 1271       | http          | -408       |\n",
      "| 22   | here      | 1166       | according     | -406       |\n",
      "| 23   | but       | 1117       | total         | -401       |\n",
      "| 24   | [         | 1095       | during        | -398       |\n",
      "| 25   | not       | 1054       | l             | -397       |\n",
      "| 26   | we        | 1047       | operations    | -396       |\n",
      "| 27   | ”         | 1011       | contact       | -395       |\n",
      "| 28   | get       | 952        | state         | -394       |\n",
      "| 29   | just      | 951        | w             | -394       |\n",
      "| 30   | how       | 951        | securities    | -388       |\n",
      "| 31   | ]         | 905        | leader        | -387       |\n",
      "| 32   | out       | 892        | investment    | -384       |\n",
      "| 33   | or        | 863        | sunday        | -383       |\n",
      "| 34   | all       | 862        | across        | -382       |\n",
      "| 35   | see       | 860        | former        | -382       |\n",
      "| 36   | |         | 845        | group         | -380       |\n",
      "| 37   | one       | 835        | world         | -378       |\n",
      "| 38   | love      | 785        | news.net      | -373       |\n",
      "| 39   | post      | 782        | years         | -369       |\n",
      "| 40   | there     | 779        | health        | -368       |\n",
      "| 41   | even      | 757        | people        | -366       |\n",
      "| 42   | them      | 727        | united        | -364       |\n",
      "| 43   | some      | 721        | county        | -362       |\n",
      "| 44   | know      | 720        | capital       | -360       |\n",
      "| 45   | when      | 717        | five          | -360       |\n",
      "| 46   | time      | 689        | four          | -357       |\n",
      "| 47   | god       | 678        | our           | -345       |\n",
      "| 48   | am        | 633        | august        | -345       |\n",
      "| 49   | free      | 609        | period        | -342       |\n",
      "| 50   | don       | 598        | local         | -342       |\n",
      "| 51   | “         | 597        | executive     | -338       |\n",
      "| 52   | good      | 582        | provides      | -337       |\n",
      "| 53   | things    | 571        | china         | -331       |\n",
      "| 54   | <         | 566        | performance   | -331       |\n",
      "| 55   | re        | 565        | while         | -330       |\n",
      "| 56   | no        | 559        | prime         | -329       |\n",
      "| 57   | now       | 542        | key           | -328       |\n",
      "| 58   | #         | 542        | net           | -326       |\n",
      "| 59   | then      | 535        | source        | -325       |\n",
      "| 60   | &         | 532        | g             | -322       |\n",
      "| 61   | these     | 531        | number        | -318       |\n",
      "| 62   | up        | 516        | win           | -316       |\n",
      "| 63   | >         | 514        | conference    | -315       |\n",
      "| 64   | really    | 495        | added         | -314       |\n",
      "| 65   | little    | 494        | expected      | -310       |\n",
      "| 66   | much      | 493        | press         | -309       |\n",
      "| 67   | too       | 481        | nasdaq        | -309       |\n",
      "| 68   | are       | 481        | firm          | -308       |\n",
      "| 69   | want      | 477        | reserved      | -302       |\n",
      "| 70   | /         | 472        | india         | -302       |\n",
      "| 71   | they      | 470        | â             | -301       |\n",
      "| 72   | make      | 455        | bank          | -300       |\n",
      "| 73   | way       | 454        | ceo           | -300       |\n",
      "| 74   | be        | 452        | clients       | -299       |\n",
      "| 75   | look      | 451        | platform      | -298       |\n",
      "| 76   | blog      | 449        | tax           | -295       |\n",
      "| 77   | because   | 440        | provide       | -295       |\n",
      "| 78   | always    | 417        | increase      | -290       |\n",
      "| 79   | let       | 414        | support       | -289       |\n",
      "| 80   | book      | 413        | security      | -289       |\n",
      "| 81   | something | 407        | six           | -287       |\n",
      "| 82   | google    | 404        | exchange      | -285       |\n",
      "| 83   | why       | 403        | association   | -284       |\n",
      "| 84   | m         | 396        | patients      | -284       |\n",
      "| 85   | only      | 396        | income        | -283       |\n",
      "| 86   | read      | 392        | p.m.          | -282       |\n",
      "| 87   | ·         | 392        | price         | -281       |\n",
      "| 88   | use       | 388        | sydney        | -279       |\n",
      "| 89   | find      | 387        | new           | -278       |\n",
      "| 90   | day       | 384        | june          | -275       |\n",
      "| 91   | great     | 383        | europe        | -275       |\n",
      "| 92   | facebook  | 380        | director      | -275       |\n",
      "| 93   | ll        | 374        | investors     | -273       |\n",
      "| 94   | life      | 370        | include       | -273       |\n",
      "| 95   | back      | 369        | reported      | -272       |\n",
      "| 96   | does      | 357        | north         | -271       |\n",
      "| 97   | ve        | 355        | rate          | -270       |\n",
      "| 98   | go        | 355        | second        | -269       |\n",
      "| 99   | feel      | 349        | future        | -269       |\n",
      "+------+-----------+------------+---------------+------------+\n"
     ]
    }
   ],
   "source": [
    "news_articles = reduced_article_db[reduced_article_db['blog'] == 0]\n",
    "blog_articles = reduced_article_db[reduced_article_db['blog'] == 1]\n",
    "  \n",
    "token_count_news = get_token_count(news_articles)\n",
    "token_count_blogs = get_token_count(blog_articles)\n",
    "\n",
    "# Get all tokens\n",
    "tokens = list(token_count_news.keys())\n",
    "tokens.extend(list(token_count_blogs.keys()))\n",
    "tokens = set(tokens)\n",
    "\n",
    "# Get the token occurence difference between news and blog articles\n",
    "diff_count = defaultdict(int)\n",
    "for token in tqdm(tokens, desc=\"Determine occurence diff.\"):\n",
    "    diff_count[token] += token_count_blogs.get(token, 0)\n",
    "    diff_count[token] -= token_count_news.get(token, 0)\n",
    "\n",
    "# Sort the tokens by the largest occurence diff. and display the 200 where\n",
    "# the difference was the largest\n",
    "sorted_diff_count = sorted(diff_count.items(), key=lambda x: x[1], reverse=True)\n",
    "first_hundred = sorted_diff_count[:100]\n",
    "last_hundred = sorted_diff_count[-100::-1]\n",
    "\n",
    "# Display the tokens with the largest difference \n",
    "# (idea is that the count of tokens with a large occurence diff will make good features)\n",
    "table_data = [[\"Num.\", \"Token\", \"Count Diff\", \"Token\", \"Count Diff\"]]\n",
    "for i, ((f_token, f_diff), (l_token, l_diff)) in enumerate(zip(first_hundred, last_hundred)):\n",
    "    table_data.append([i, f_token, f_diff, l_token, l_diff])\n",
    "print (AsciiTable(table_data).table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens whose occurence count will act as features\n",
    "# Selected based on table above\n",
    "tokens_to_check = [\"i\", \n",
    "                   \"you\", \n",
    "                   \"me\", \n",
    "                   \"we\", \n",
    "                   \"my\", \n",
    "                   \"this\", \n",
    "                   \"that\", \n",
    "                   \"it\", \n",
    "                   \"like\", \\\n",
    "                   \"--\", \n",
    "                   \"-\", \n",
    "                   \"''\", \n",
    "                   \"%\", \n",
    "                   \"said\", \n",
    "                   \"?\", \n",
    "                   \"!\", \n",
    "                   \"’\", \n",
    "                   \":\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building feature set: 100%|██████████| 10000/10000 [02:00<00:00, 82.74it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "feature_set = []\n",
    "\n",
    "for i in tqdm(range(reduced_article_db.shape[0]), desc=\"Building feature set\"):\n",
    "    article = reduced_article_db.iloc[i]\n",
    "    article_text = article[\"content\"]\n",
    "    \n",
    "    article_features = {}\n",
    "    article_features[\"article_length\"] = len(article_text)\n",
    "    \n",
    "    # Get token count in the article\n",
    "    tokens = nltk.word_tokenize(article_text.lower())\n",
    "    token_counts = dict(Counter(tokens))\n",
    "\n",
    "    # Token count features\n",
    "    for token in tokens_to_check:\n",
    "        # Get occurence in this article\n",
    "        occurence = token_counts.get(token, 0)\n",
    "        article_features[token] = occurence\n",
    "        \n",
    "    # Sentiment features\n",
    "    sentiment = sid.polarity_scores(article_text)\n",
    "    article_features[\"positivity\"] = sentiment['pos']\n",
    "    article_features[\"negativity\"] = sentiment['neg']\n",
    "    article_features[\"neutral\"] = sentiment['neu']\n",
    "    article_features[\"compound\"] = sentiment['compound']\n",
    "    \n",
    "    # Target value\n",
    "    article_features[\"blog\"] = article[\"blog\"]\n",
    "    \n",
    "    feature_set.append(article_features)\n",
    "    \n",
    "dataset = pd.DataFrame(feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "dataset.to_pickle(\"data/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 24)\n"
     ]
    }
   ],
   "source": [
    "print (dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
