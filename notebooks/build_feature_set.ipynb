{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from terminaltables import AsciiTable\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "PATH_TO_DATA = '../data'\n",
    "\n",
    "article_db = None\n",
    "reduced_article_db = None\n",
    "\n",
    "os.makedirs(PATH_TO_DATA, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reduced article DB...\n",
      "+ Done.\n"
     ]
    }
   ],
   "source": [
    "# Load reduced article db\n",
    "if os.path.exists('%s/articles' % PATH_TO_DATA):\n",
    "    print (\"Loading reduced article DB...\")\n",
    "    reduced_article_db = pd.read_pickle('%s/articles' % PATH_TO_DATA)\n",
    "    print (\"+ Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading articles: 100%|██████████| 1000000/1000000 [01:07<00:00, 14827.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load as Pandas DF...\n",
      "+ Done.\n"
     ]
    }
   ],
   "source": [
    "signalmedia_url = 'http://research.signalmedia.co/newsir16/signal-dataset.html'\n",
    "\n",
    "# Assert that dataset file is downloaded and in 'data/'\n",
    "assert os.path.exists('%s/signalmedia-1m.jsonl' % PATH_TO_DATA), \\\n",
    "        \"Please download SignalMedia dataset from %s and save it to 'data/'\\\n",
    "         before executing this cell.\" % signalmedia_url\n",
    "\n",
    "# Load full article DB\n",
    "lines = []\n",
    "with open('%s/signalmedia-1m.jsonl' % PATH_TO_DATA, 'r') as f:\n",
    "    for json_article in tqdm(f.readlines(), desc='Loading articles'):\n",
    "        article = dict(json.loads(json_article))\n",
    "        # Extract article id, content and media type\n",
    "        lines.append({'id': article['id'], \n",
    "                      'content': article['content'], \n",
    "                      'media-type': article['media-type']})\n",
    "\n",
    "print (\"Load as Pandas DF...\")\n",
    "article_db = pd.DataFrame(lines)\n",
    "print (\"+ Done.\")\n",
    "\n",
    "# One-hot encoding of 'media-type': 'News' = 0, 'Blog = 1\n",
    "article_db.loc[article_db['media-type'] == 'News', 'blog'] = 0\n",
    "article_db.loc[article_db['media-type'] == 'Blog', 'blog'] = 1\n",
    "\n",
    "# Remove rows that has NaN values\n",
    "article_db.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      "+ content\n",
      "+ id\n",
      "+ media-type\n",
      "+ blog\n"
     ]
    }
   ],
   "source": [
    "assert article_db is not None or reduced_article_db is not None, \\\n",
    "    \"Load either the full article dataset or the reduced article \\\n",
    "    dataset before executing this cell.\"\n",
    "\n",
    "# Extract column names\n",
    "cols = article_db.columns if article_db is not None else reduced_article_db.columns\n",
    "\n",
    "# Show column names\n",
    "print (\"Columns:\")\n",
    "for col in cols:\n",
    "    print (\"+ %s\" % col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce and balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article DB\n",
      "+------------+--------+\n",
      "| Media Type | Count  |\n",
      "+------------+--------+\n",
      "| News       | 734488 |\n",
      "| Blog       | 265512 |\n",
      "+------------+--------+\n",
      "\n",
      "Reduced Article DB\n",
      "+------------+-------+\n",
      "| Media Type | Count |\n",
      "+------------+-------+\n",
      "| News       | 10000 |\n",
      "| Blog       | 10000 |\n",
      "+------------+-------+\n"
     ]
    }
   ],
   "source": [
    "def print_media_occ(article_type, articles):\n",
    "    \"\"\"Count number of occurences of each class and print as a table\"\"\"\n",
    "    print (article_type)\n",
    "    num_news = np.sum(articles['blog'] == 0)\n",
    "    num_blogs = np.sum(articles['blog'] == 1)\n",
    "    \n",
    "    table_data = [[\"Media Type\", \"Count\"]]\n",
    "    for media_type, count in [(\"News\", num_news), (\"Blog\", num_blogs)]:\n",
    "        table_data.append([media_type, count])\n",
    "    print (AsciiTable(table_data).table)\n",
    "    \n",
    "if article_db is not None:\n",
    "    # Display media type occurences of the original dataset\n",
    "    print_media_occ('Article DB', article_db)\n",
    "    print ()\n",
    "\n",
    "if reduced_article_db is None and article_db is not None:\n",
    "    print (\"Reduce and balance dataset...\")\n",
    "    n_of_each = 10000\n",
    "    news_articles = article_db[article_db['blog'] == 0].iloc[:n_of_each]\n",
    "    blog_articles = article_db[article_db['blog'] == 1].iloc[:n_of_each]\n",
    "    reduced_article_db = pd.concat([news_articles, blog_articles])\n",
    "    print (\"+ Done.\\n\")\n",
    "        \n",
    "    reduced_article_db.to_pickle(\"data/articles\")\n",
    "    \n",
    "# Display media type occurences of the reduced and balance dataset\n",
    "print_media_occ('Reduced Article DB', reduced_article_db)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Lemmatizing (the proper way, accounting for different POS tags)\n",
    "def penn_to_wn(penn_tag):\n",
    "    \"\"\"\n",
    "    Returns the corresponding WordNet POS tag for a Penn TreeBank POS tag.\n",
    "    \"\"\"\n",
    "    if penn_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        wn_tag = wn.NOUN\n",
    "    elif penn_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        wn_tag = wn.VERB\n",
    "    elif penn_tag in ['RB', 'RBR', 'RBS']:\n",
    "        wn_tag = wn.ADV\n",
    "    elif penn_tag in ['JJ', 'JJR', 'JJS']:\n",
    "        wn_tag = wn.ADJ\n",
    "    else:\n",
    "        wn_tag = None\n",
    "    return wn_tag\n",
    "\n",
    "def get_lemmas(tokens):\n",
    "    \"\"\"Lemmatize the list of tokens\"\"\"\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmas = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        wn_tag = penn_to_wn(pos)\n",
    "        lemma = lmtzr.lemmatize(token) if not wn_tag else lmtzr.lemmatize(token, wn_tag)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def token_count_in_articles(articles):\n",
    "    \"\"\"\n",
    "    Return occurence count of tokens in all articles.\n",
    "\n",
    "    :param DataFrame articles: Collection of articles\n",
    "    :returns int: A count value\n",
    "    \"\"\"\n",
    "    total_count = defaultdict(int)\n",
    "    for i in tqdm(range(len(articles)), desc='Counting token occurences'):\n",
    "        text = articles.iloc[i]['content'].lower()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        token_count = get_token_count(tokens)\n",
    "        for token, count in token_count.items():\n",
    "            total_count[token] += count\n",
    "    return total_count\n",
    "\n",
    "def get_token_count(tokens):\n",
    "    \"\"\"\n",
    "    Count occurences of tokens in text.\n",
    "\n",
    "    :param list tokens: List of tokens\n",
    "    :returns dict: Dict of tokens and their count\n",
    "    \"\"\"\n",
    "    token_counts = dict(Counter(tokens))\n",
    "    return token_counts\n",
    "\n",
    "def get_spelling_errors(text):\n",
    "    \"\"\"\n",
    "    Count number of spelling errors in text.\n",
    "\n",
    "    :param str text\n",
    "    :returns int: Number of spelling errors\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    cnt = 0\n",
    "    for token in tokens:\n",
    "        # If spelling error and token is not a stop word\n",
    "        if not (wn.synsets(token) and token in stop_words_en):\n",
    "            cnt += 1\n",
    "    return cnt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display discrepancies of token occurences between media types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting token occurences: 100%|██████████| 40000/40000 [03:51<00:00, 173.08it/s]\n",
      "Counting token occurences: 100%|██████████| 40000/40000 [04:39<00:00, 142.95it/s]\n",
      "Determine occurence diff.: 100%|██████████| 552835/552835 [00:01<00:00, 368255.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+\n",
      "| Tokens with largest occurence diff. between media types |\n",
      "+---------------------------------------------------------+\n",
      "- Positive values indicate a higher occurence in blogs than news\n",
      "+------+-----------+------------+---------------+------------+\n",
      "| Num. | Token     | Count Diff | Token         | Count Diff |\n",
      "+------+-----------+------------+---------------+------------+\n",
      "| 0    | i         | 73575      | against       | -3137      |\n",
      "| 1    | you       | 68140      | markets       | -3129      |\n",
      "| 2    | ’         | 61844      | capital       | -3039      |\n",
      "| 3    | !         | 37349      | national      | -3037      |\n",
      "| 4    | :         | 35841      | source        | -3029      |\n",
      "| 5    | it        | 34568      | %             | -2991      |\n",
      "| 6    | your      | 30384      | announced     | -2988      |\n",
      "| 7    | my        | 27277      | cent          | -2985      |\n",
      "| 8    | s         | 26736      | according     | -2974      |\n",
      "| 9    | this      | 26374      | media         | -2940      |\n",
      "| 10   | that      | 21915      | key           | -2936      |\n",
      "| 11   | ?         | 21519      | told          | -2936      |\n",
      "| 12   | is        | 17363      | customers     | -2932      |\n",
      "| 13   | so        | 16328      | such          | -2921      |\n",
      "| 14   | if        | 14648      | former        | -2905      |\n",
      "| 15   | t         | 14356      | country       | -2870      |\n",
      "| 16   | can       | 13983      | were          | -2848      |\n",
      "| 17   | –         | 13634      | contact       | -2844      |\n",
      "| 18   | me        | 13231      | international | -2752      |\n",
      "| 19   | what      | 12803      | two           | -2737      |\n",
      "| 20   | we        | 11811      | team          | -2694      |\n",
      "| 21   | do        | 11733      | years         | -2689      |\n",
      "| 22   | like      | 11232      | provides      | -2676      |\n",
      "| 23   | but       | 10777      | china         | -2671      |\n",
      "| 24   | not       | 10488      | north         | -2664      |\n",
      "| 25   | all       | 10210      | quarter       | -2660      |\n",
      "| 26   | here      | 9409       | leader        | -2658      |\n",
      "| 27   | )         | 8731       | during        | -2652      |\n",
      "| 28   | how       | 8592       | india         | -2649      |\n",
      "| 29   | [         | 8570       | press         | -2631      |\n",
      "| 30   | |         | 8171       | than          | -2618      |\n",
      "| 31   | ”         | 8114       | across        | -2614      |\n",
      "| 32   | ]         | 8068       | four          | -2610      |\n",
      "| 33   | get       | 8016       | state         | -2555      |\n",
      "| 34   | just      | 7957       | support       | -2544      |\n",
      "| 35   | see       | 7470       | p.m.          | -2541      |\n",
      "| 36   | out       | 7410       | news.net      | -2487      |\n",
      "| 37   | one       | 7099       | reserved      | -2485      |\n",
      "| 38   | there     | 6956       | executive     | -2477      |\n",
      "| 39   | or        | 6936       | five          | -2465      |\n",
      "| 40   | (         | 6841       | under         | -2461      |\n",
      "| 41   | post      | 6790       | public        | -2408      |\n",
      "| 42   | time      | 6776       | results       | -2406      |\n",
      "| 43   | some      | 6757       | sydney        | -2390      |\n",
      "| 44   | >         | 6741       | expected      | -2382      |\n",
      "| 45   | #         | 6489       | nasdaq        | -2366      |\n",
      "| 46   | up        | 6325       | sunday        | -2363      |\n",
      "| 47   | when      | 6088       | city          | -2334      |\n",
      "| 48   | love      | 6079       | south         | -2326      |\n",
      "| 49   | no        | 6044       | reports       | -2318      |\n",
      "| 50   | even      | 5823       | include       | -2315      |\n",
      "| 51   | know      | 5801       | provide       | -2309      |\n",
      "| 52   | then      | 5783       | performance   | -2290      |\n",
      "| 53   | them      | 5513       | prime         | -2289      |\n",
      "| 54   | dragon    | 5356       | director      | -2265      |\n",
      "| 55   | “         | 5321       | exchange      | -2265      |\n",
      "| 56   | &         | 5245       | local         | -2240      |\n",
      "| 57   | god       | 5122       | senior        | -2238      |\n",
      "| 58   | be        | 5120       | england       | -2222      |\n",
      "| 59   | 1         | 5034       | operations    | -2201      |\n",
      "| 60   | good      | 4858       | community     | -2198      |\n",
      "| 61   | now       | 4827       | court         | -2195      |\n",
      "| 62   | make      | 4727       | bank          | -2168      |\n",
      "| 63   | don       | 4706       | firm          | -2155      |\n",
      "| 64   | they      | 4693       | corporation   | -2152      |\n",
      "| 65   | re        | 4650       | countries     | -2148      |\n",
      "| 66   | only      | 4498       | other         | -2147      |\n",
      "| 67   | am        | 4494       | patients      | -2138      |\n",
      "| 68   | much      | 4407       | future        | -2131      |\n",
      "| 69   | ball      | 4345       | officer       | -2125      |\n",
      "| 70   | free      | 4288       | analysts      | -2098      |\n",
      "| 71   | really    | 4229       | release       | -2097      |\n",
      "| 72   | because   | 4168       | association   | -2095      |\n",
      "| 73   | little    | 4152       | further       | -2094      |\n",
      "| 74   | things    | 4015       | six           | -2070      |\n",
      "| 75   | why       | 3885       | medical       | -2065      |\n",
      "| 76   | something | 3782       | average       | -2044      |\n",
      "| 77   | too       | 3775       | council       | -2021      |\n",
      "| 78   | these     | 3756       | revenue       | -2020      |\n",
      "| 79   | want      | 3674       | risk          | -2017      |\n",
      "| 80   | book      | 3640       | players       | -2016      |\n",
      "| 81   | back      | 3615       | high          | -2015      |\n",
      "| 82   | 2         | 3615       | risks         | -2006      |\n",
      "| 83   | day       | 3530       | second        | -2000      |\n",
      "| 84   | have      | 3525       | annual        | -1998      |\n",
      "| 85   | way       | 3523       | economic      | -1995      |\n",
      "| 86   | m         | 3512       | nyse          | -1993      |\n",
      "| 87   | let       | 3432       | reported      | -1965      |\n",
      "| 88   | ‘         | 3414       | number        | -1954      |\n",
      "| 89   | use       | 3378       | added         | -1950      |\n",
      "| 90   | great     | 3327       | security      | -1949      |\n",
      "| 91   | blog      | 3318       | york          | -1948      |\n",
      "| 92   | go        | 3312       | ceo           | -1914      |\n",
      "| 93   | think     | 3308       | largest       | -1911      |\n",
      "| 94   | ll        | 3305       | major         | -1888      |\n",
      "| 95   | find      | 3289       | statement     | -1864      |\n",
      "| 96   | ve        | 3266       | states        | -1859      |\n",
      "| 97   | very      | 3259       | had           | -1853      |\n",
      "| 98   | look      | 3213       | investors     | -1848      |\n",
      "| 99   | 3         | 3206       | analysis      | -1847      |\n",
      "+------+-----------+------------+---------------+------------+\n"
     ]
    }
   ],
   "source": [
    "assert article_db is not None or reduced_article_db is not None, \\\n",
    "    \"Load either the full article dataset or the reduced article \\\n",
    "    dataset before executing this cell.\"\n",
    "\n",
    "if article_db is not None:\n",
    "    # Randomly select 40 000 articles of each type and measure token occurence discrepancy\n",
    "    idx = np.random.randint(0, 200000, size=40000)\n",
    "    news_articles = article_db[article_db['media-type'] == 'News'].iloc[idx]\n",
    "    blog_articles = article_db[article_db['media-type'] == 'Blog'].iloc[idx]\n",
    "else:\n",
    "    # Load reduced article dataset\n",
    "    news_articles = reduced_article_db[reduced_article_db['blog'] == 0]\n",
    "    blog_articles = reduced_article_db[reduced_article_db['blog'] == 1]\n",
    "    \n",
    "token_count_news = token_count_in_articles(news_articles)\n",
    "token_count_blogs = token_count_in_articles(blog_articles)\n",
    "\n",
    "# Get all tokens\n",
    "tokens = set(token_count_news.keys())\n",
    "tokens.update(list(token_count_blogs.keys()))\n",
    "\n",
    "# Get the token occurence difference between news and blog articles\n",
    "diff_count = defaultdict(int)\n",
    "for token in tqdm(tokens, desc='Determine occurence diff.'):\n",
    "    diff_count[token] += token_count_blogs.get(token, 0)\n",
    "    diff_count[token] -= token_count_news.get(token, 0)\n",
    "\n",
    "# Sort the tokens by the largest occurence difference\n",
    "sorted_diff_count = sorted(diff_count.items(), key=lambda x: x[1], reverse=True)\n",
    "first_hundred = sorted_diff_count[:100]     # Higher occurence in blogs than news\n",
    "last_hundred = sorted_diff_count[-100::-1]  # Higher occurence in news than blogs\n",
    "\n",
    "# Display the tokens with the largest difference \n",
    "# (idea is that the count of tokens with a large occurence diff will make good features)\n",
    "print (AsciiTable([['Tokens with largest occurence diff. between media types']]).table)\n",
    "print (\"- Positive values indicate a higher occurence in blogs than news\")\n",
    "table_data = [['Num.', 'Token', 'Count Diff', 'Token', 'Count Diff']]\n",
    "for i in range(len(first_hundred)):\n",
    "    f_token, f_diff = first_hundred[i]\n",
    "    l_token, l_diff = last_hundred[i]\n",
    "    table_data.append([i, f_token, f_diff, l_token, l_diff])\n",
    "print (AsciiTable(table_data).table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cherry-picked tokens whose occurence count will act as features\n",
    "tokens_to_check = ['i', \n",
    "                   'you', \n",
    "                   'me', \n",
    "                   'we', \n",
    "                   'my', \n",
    "                   'this', \n",
    "                   'that', \n",
    "                   'it', \n",
    "                   'like',\n",
    "                   '--', \n",
    "                   '-', \n",
    "                   \"''\", \n",
    "                   '%', \n",
    "                   'said',\n",
    "                   'told',\n",
    "                   '?', \n",
    "                   '!', \n",
    "                   '’', \n",
    "                   ':']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building feature set: 100%|██████████| 20000/20000 [09:24<00:00, 35.40it/s]\n"
     ]
    }
   ],
   "source": [
    "assert reduced_article_db is not None, \\\n",
    "    \"Load a reduced and balanced version of the original dataset\\\n",
    "     before executing this cell.\"\n",
    "    \n",
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "feature_set = []\n",
    "for i in tqdm(range(reduced_article_db.shape[0]), desc='Building feature set'):\n",
    "    # Extract the article\n",
    "    article = reduced_article_db.iloc[i]\n",
    "    \n",
    "    # Extract the article content\n",
    "    article_text = article['content']\n",
    "    \n",
    "    # Extract tokens and sentences from the text\n",
    "    tokens = nltk.word_tokenize(article_text.lower())\n",
    "    sentences = nltk.sent_tokenize(article_text.lower())\n",
    "    \n",
    "    article_features = {}\n",
    "    article_features['article_length'] = len(article_text)\n",
    "    # Average token length\n",
    "    article_features['token_length'] = sum(len(t) for t in tokens) / len(tokens)\n",
    "    # Spelling errors per token\n",
    "    article_features['spelling_errors'] = get_spelling_errors(article_text) / len(tokens)\n",
    "    \n",
    "    # Get token count in the article\n",
    "    token_counts = get_token_count(tokens)\n",
    "\n",
    "    # Token occurence features\n",
    "    for token in tokens_to_check:\n",
    "        article_features[token] = token_counts.get(token, 0)\n",
    "      \n",
    "    # Sentiment features, averaged per sentence\n",
    "    sentiments = {'pos': 0, 'neg': 0, 'neu': 0, 'comp': 0}\n",
    "    for sentence in sentences:\n",
    "        sentiment = sid.polarity_scores(sentence)\n",
    "        sentiments['pos'] += sentiment['pos']\n",
    "        sentiments['neg'] += sentiment['neg']\n",
    "        sentiments['neu'] += sentiment['neu']\n",
    "        sentiments['comp'] += sentiment['compound']\n",
    "    \n",
    "    # Save the average sentiment scores\n",
    "    article_features['sent_pos'] = sentiments['pos'] / len(sentences)\n",
    "    article_features['sent_neg'] = sentiments['neg'] / len(sentences)\n",
    "    article_features['sent_neu'] = sentiments['neu'] / len(sentences)\n",
    "    article_features['sent_comp'] = sentiments['comp'] / len(sentences)\n",
    "    \n",
    "    # Target value\n",
    "    article_features['blog'] = article['blog']\n",
    "    \n",
    "    feature_set.append(article_features)\n",
    "    \n",
    "dataset = pd.DataFrame(feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "dataset.to_pickle('%s/dataset' % PATH_TO_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
