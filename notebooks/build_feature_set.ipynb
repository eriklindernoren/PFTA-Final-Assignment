{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from terminaltables import AsciiTable\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "PATH_TO_DATA = '../data'\n",
    "\n",
    "article_db = None\n",
    "reduced_article_db = None\n",
    "\n",
    "os.makedirs(PATH_TO_DATA, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reduced article DB...\n",
      "+ Done.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('%s/articles' % PATH_TO_DATA):\n",
    "    # Load saved reduced article db\n",
    "    print (\"Loading reduced article DB...\")\n",
    "    reduced_article_db = pd.read_pickle('%s/articles' % PATH_TO_DATA)\n",
    "    print (\"+ Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading articles: 100%|██████████| 1000000/1000000 [00:58<00:00, 17105.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load as Pandas DF...\n",
      "+ Done.\n"
     ]
    }
   ],
   "source": [
    "# Load full article DB\n",
    "lines = []\n",
    "with open('%s/signalmedia-1m.jsonl' % PATH_TO_DATA, 'r') as f:\n",
    "    for json_article in tqdm(f.readlines(), desc='Loading articles'):\n",
    "        article = dict(json.loads(json_article))\n",
    "        # Extract article id, content and media type\n",
    "        lines.append({'id': article['id'], \n",
    "                      'content': article['content'], \n",
    "                      'media-type': article['media-type']})\n",
    "\n",
    "print (\"Load as Pandas DF...\")\n",
    "article_db = pd.DataFrame(lines)\n",
    "print (\"+ Done.\")\n",
    "\n",
    "# One-hot encoding of 'media-type': 'News' = 0, 'Blog = 1\n",
    "article_db.loc[article_db['media-type'] == 'News', 'blog'] = 0\n",
    "article_db.loc[article_db['media-type'] == 'Blog', 'blog'] = 1\n",
    "\n",
    "# Remove rows that has NaN values\n",
    "article_db.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      "+ content\n",
      "+ id\n",
      "+ media-type\n",
      "+ blog\n"
     ]
    }
   ],
   "source": [
    "# Show column names\n",
    "print (\"Columns:\")\n",
    "for col in article_db.columns:\n",
    "    print (\"+ %s\" % col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce and balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article DB\n",
      "+------------+--------+\n",
      "| Media Type | Count  |\n",
      "+------------+--------+\n",
      "| News       | 734488 |\n",
      "| Blog       | 265512 |\n",
      "+------------+--------+\n",
      "\n",
      "Reduced Article DB\n",
      "+------------+-------+\n",
      "| Media Type | Count |\n",
      "+------------+-------+\n",
      "| News       | 10000 |\n",
      "| Blog       | 10000 |\n",
      "+------------+-------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_media_occ(article_type, articles):\n",
    "    \"\"\"Count number of occurences of each class and print as a table\"\"\"\n",
    "    print (article_type)\n",
    "    num_news = np.sum(articles['blog'] == 0)\n",
    "    num_blogs = np.sum(articles['blog'] == 1)\n",
    "    \n",
    "    table_data = [[\"Media Type\", \"Count\"]]\n",
    "    for media_type, count in [(\"News\", num_news), (\"Blog\", num_blogs)]:\n",
    "        table_data.append([media_type, count])\n",
    "    print (AsciiTable(table_data).table)\n",
    "  \n",
    "\n",
    "print_media_occ('Article DB', article_db)\n",
    "print ()\n",
    "\n",
    "if reduced_article_db is None:\n",
    "    \n",
    "    print (\"\\nReduce and balance dataset...\")\n",
    "    n_of_each = 10000\n",
    "    news_articles = article_db[article_db['blog'] == 0].iloc[:n_of_each]\n",
    "    blog_articles = article_db[article_db['blog'] == 1].iloc[:n_of_each]\n",
    "    reduced_article_db = pd.concat([news_articles, blog_articles])\n",
    "    print (\"+ Done.\\n\")\n",
    "    \n",
    "    print_media_occ(reduced_article_db)\n",
    "    \n",
    "    reduced_article_db.to_pickle(\"data/articles\")\n",
    "    \n",
    "else:\n",
    "    print_media_occ('Reduced Article DB', reduced_article_db)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Lemmatizing (the proper way, accounting for different POS tags)\n",
    "def penn_to_wn(penn_tag):\n",
    "    \"\"\"\n",
    "    Returns the corresponding WordNet POS tag for a Penn TreeBank POS tag.\n",
    "    \"\"\"\n",
    "    if penn_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        wn_tag = wn.NOUN\n",
    "    elif penn_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        wn_tag = wn.VERB\n",
    "    elif penn_tag in ['RB', 'RBR', 'RBS']:\n",
    "        wn_tag = wn.ADV\n",
    "    elif penn_tag in ['JJ', 'JJR', 'JJS']:\n",
    "        wn_tag = wn.ADJ\n",
    "    else:\n",
    "        wn_tag = None\n",
    "    return wn_tag\n",
    "\n",
    "def get_lemmas(tokens):\n",
    "    \"\"\"Lemmatize the list of tokens\"\"\"\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmas = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        wn_tag = penn_to_wn(pos)\n",
    "        lemma = lmtzr.lemmatize(token) if not wn_tag else lmtzr.lemmatize(token, wn_tag)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def token_count_in_articles(articles):\n",
    "    \"\"\"\n",
    "    Return occurence count of tokens in all articles.\n",
    "\n",
    "    :param DataFrame articles: Collection of articles\n",
    "    :returns int: A count value\n",
    "    \"\"\"\n",
    "    total_count = defaultdict(int)\n",
    "    for i in tqdm(range(len(articles)), desc='Counting token occurences'):\n",
    "        text = articles.iloc[i]['content'].lower()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        token_count = get_token_count(tokens)\n",
    "        for token, count in token_count.items():\n",
    "            total_count[token] += count\n",
    "    return total_count\n",
    "\n",
    "def get_token_count(tokens):\n",
    "    \"\"\"\n",
    "    Count occurences of tokens in text.\n",
    "\n",
    "    :param list tokens: List of tokens\n",
    "    :returns dict: Dict of tokens and their count\n",
    "    \"\"\"\n",
    "    token_counts = dict(Counter(tokens))\n",
    "    return token_counts\n",
    "\n",
    "def get_spelling_errors(text):\n",
    "    \"\"\"\n",
    "    Count number of spelling errors in text.\n",
    "\n",
    "    :param str text\n",
    "    :returns int: Number of spelling errors\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    cnt = 0\n",
    "    for token in tokens:\n",
    "        # If spelling error and token is not a stop word\n",
    "        if not (wn.synsets(token) and token in stop_words_en):\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display discrepancies of token occurences between media types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting token occurences: 100%|██████████| 40000/40000 [03:34<00:00, 186.69it/s]\n",
      "Counting token occurences: 100%|██████████| 40000/40000 [05:06<00:00, 130.58it/s]\n",
      "Determine occurence diff.: 100%|██████████| 548931/548931 [00:01<00:00, 341736.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+\n",
      "| Tokens with largest occurence diff. between media types |\n",
      "+---------------------------------------------------------+\n",
      "- Positive values indicate a higher occurence in blogs than news\n",
      "+------+-----------+------------+---------------+------------+\n",
      "| Num. | Token     | Count Diff | Token         | Count Diff |\n",
      "+------+-----------+------------+---------------+------------+\n",
      "| 0    | i         | 74279      | markets       | -3313      |\n",
      "| 1    | you       | 68589      | cent          | -3304      |\n",
      "| 2    | ’         | 58196      | china         | -3266      |\n",
      "| 3    | !         | 34868      | former        | -3260      |\n",
      "| 4    | it        | 33865      | over          | -3254      |\n",
      "| 5    | :         | 32741      | told          | -3254      |\n",
      "| 6    | your      | 29974      | securities    | -3246      |\n",
      "| 7    | my        | 27386      | thursday      | -3242      |\n",
      "| 8    | s         | 24706      | source        | -3199      |\n",
      "| 9    | this      | 24705      | years         | -3179      |\n",
      "| 10   | ?         | 21673      | capital       | -3178      |\n",
      "| 11   | that      | 19860      | announced     | -3119      |\n",
      "| 12   | is        | 15805      | rights        | -3046      |\n",
      "| 13   | so        | 15493      | country       | -3039      |\n",
      "| 14   | if        | 14316      | national      | -3021      |\n",
      "| 15   | t         | 13754      | media         | -2989      |\n",
      "| 16   | can       | 13534      | saturday      | -2982      |\n",
      "| 17   | me        | 13262      | such          | -2973      |\n",
      "| 18   | –         | 12354      | united        | -2940      |\n",
      "| 19   | what      | 12156      | contact       | -2923      |\n",
      "| 20   | like      | 11522      | according     | -2918      |\n",
      "| 21   | do        | 10636      | results       | -2903      |\n",
      "| 22   | but       | 10414      | during        | -2891      |\n",
      "| 23   | we        | 9382       | key           | -2884      |\n",
      "| 24   | not       | 9176       | news          | -2855      |\n",
      "| 25   | all       | 9155       | customers     | -2851      |\n",
      "| 26   | [         | 9074       | leader        | -2816      |\n",
      "| 27   | here      | 8992       | average       | -2792      |\n",
      "| 28   | ]         | 8695       | international | -2748      |\n",
      "| 29   | how       | 8681       | executive     | -2717      |\n",
      "| 30   | |         | 8362       | provides      | -2689      |\n",
      "| 31   | get       | 8323       | india         | -2687      |\n",
      "| 32   | out       | 7875       | city          | -2685      |\n",
      "| 33   | just      | 7732       | bank          | -2668      |\n",
      "| 34   | one       | 7707       | sunday        | -2622      |\n",
      "| 35   | or        | 7587       | had           | -2620      |\n",
      "| 36   | see       | 7456       | public        | -2619      |\n",
      "| 37   | some      | 7049       | north         | -2618      |\n",
      "| 38   | there     | 6737       | quarter       | -2610      |\n",
      "| 39   | post      | 6507       | across        | -2575      |\n",
      "| 40   | then      | 6197       | four          | -2559      |\n",
      "| 41   | >         | 6081       | south         | -2551      |\n",
      "| 42   | up        | 6049       | court         | -2536      |\n",
      "| 43   | them      | 6006       | sales         | -2521      |\n",
      "| 44   | love      | 5938       | future        | -2500      |\n",
      "| 45   | time      | 5917       | community     | -2498      |\n",
      "| 46   | know      | 5852       | support       | -2498      |\n",
      "| 47   | ”         | 5650       | nasdaq        | -2488      |\n",
      "| 48   | )         | 5537       | expected      | -2485      |\n",
      "| 49   | when      | 5488       | five          | -2483      |\n",
      "| 50   | even      | 5460       | performance   | -2441      |\n",
      "| 51   | #         | 5412       | .             | -2439      |\n",
      "| 52   | god       | 5079       | reserved      | -2435      |\n",
      "| 53   | good      | 5065       | director      | -2393      |\n",
      "| 54   | no        | 4789       | p.m.          | -2393      |\n",
      "| 55   | am        | 4784       | include       | -2388      |\n",
      "| 56   | free      | 4656       | reports       | -2372      |\n",
      "| 57   | make      | 4559       | analysts      | -2370      |\n",
      "| 58   | re        | 4536       | press         | -2350      |\n",
      "| 59   | much      | 4392       | added         | -2335      |\n",
      "| 60   | because   | 4380       | sydney        | -2334      |\n",
      "| 61   | now       | 4308       | people        | -2320      |\n",
      "| 62   | they      | 4234       | provide       | -2306      |\n",
      "| 63   | don       | 4189       | local         | -2287      |\n",
      "| 64   | little    | 4094       | news.net      | -2276      |\n",
      "| 65   | things    | 4075       | prime         | -2271      |\n",
      "| 66   | really    | 4043       | investors     | -2262      |\n",
      "| 67   | book      | 3946       | senior        | -2258      |\n",
      "| 68   | 1         | 3936       | medical       | -2258      |\n",
      "| 69   | these     | 3819       | under         | -2248      |\n",
      "| 70   | too       | 3752       | countries     | -2235      |\n",
      "| 71   | why       | 3742       | association   | -2216      |\n",
      "| 72   | let       | 3679       | number        | -2216      |\n",
      "| 73   | only      | 3673       | firm          | -2212      |\n",
      "| 74   | be        | 3655       | economic      | -2208      |\n",
      "| 75   | &         | 3623       | officer       | -2198      |\n",
      "| 76   | (         | 3587       | operations    | -2187      |\n",
      "| 77   | something | 3586       | energy        | -2160      |\n",
      "| 78   | ;         | 3505       | patients      | -2115      |\n",
      "| 79   | way       | 3485       | annual        | -2111      |\n",
      "| 80   | find      | 3430       | release       | -2106      |\n",
      "| 81   | day       | 3420       | further       | -2105      |\n",
      "| 82   | want      | 3340       | systems       | -2103      |\n",
      "| 83   | back      | 3325       | council       | -2078      |\n",
      "| 84   | ve        | 3316       | security      | -2067      |\n",
      "| 85   | blog      | 3300       | corporation   | -2050      |\n",
      "| 86   | right     | 3280       | net           | -2032      |\n",
      "| 87   | go        | 3253       | statement     | -2032      |\n",
      "| 88   | great     | 3217       | analysis      | -2030      |\n",
      "| 89   | life      | 3215       | six           | -2027      |\n",
      "| 90   | us        | 3213       | ceo           | -2019      |\n",
      "| 91   | use       | 3204       | office        | -1991      |\n",
      "| 92   | “         | 3170       | board         | -1988      |\n",
      "| 93   | look      | 3134       | england       | -1987      |\n",
      "| 94   | m         | 3100       | she           | -1986      |\n",
      "| 95   | think     | 3078       | conference    | -1972      |\n",
      "| 96   | few       | 3037       | major         | -1931      |\n",
      "| 97   | ‘         | 3018       | county        | -1931      |\n",
      "| 98   | read      | 2950       | available     | -1925      |\n",
      "| 99   | 2         | 2946       | exchange      | -1921      |\n",
      "+------+-----------+------------+---------------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 40 000 articles of each type and measure token occurence discrepancy\n",
    "\n",
    "idx = np.random.randint(0, 200000, size=40000)\n",
    "\n",
    "news_articles = article_db[article_db['media-type'] == 'News'].iloc[idx]\n",
    "blog_articles = article_db[article_db['media-type'] == 'Blog'].iloc[idx]\n",
    "  \n",
    "token_count_news = token_count_in_articles(news_articles)\n",
    "token_count_blogs = token_count_in_articles(blog_articles)\n",
    "\n",
    "# Get all tokens\n",
    "tokens = set(token_count_news.keys())\n",
    "tokens.update(list(token_count_blogs.keys()))\n",
    "\n",
    "# Get the token occurence difference between news and blog articles\n",
    "diff_count = defaultdict(int)\n",
    "for token in tqdm(tokens, desc='Determine occurence diff.'):\n",
    "    diff_count[token] += token_count_blogs.get(token, 0)\n",
    "    diff_count[token] -= token_count_news.get(token, 0)\n",
    "\n",
    "# Sort the tokens by the largest occurence difference\n",
    "sorted_diff_count = sorted(diff_count.items(), key=lambda x: x[1], reverse=True)\n",
    "first_hundred = sorted_diff_count[:100]\n",
    "last_hundred = sorted_diff_count[-100::-1]\n",
    "\n",
    "# Display the tokens with the largest difference \n",
    "# (idea is that the count of tokens with a large occurence diff will make good features)\n",
    "print (AsciiTable([['Tokens with largest occurence diff. between media types']]).table)\n",
    "print (\"- Positive values indicate a higher occurence in blogs than news\")\n",
    "table_data = [['Num.', 'Token', 'Count Diff', 'Token', 'Count Diff']]\n",
    "for i in range(len(first_hundred)):\n",
    "    f_token, f_diff = first_hundred[i]\n",
    "    l_token, l_diff = last_hundred[i]\n",
    "    table_data.append([i, f_token, f_diff, l_token, l_diff])\n",
    "print (AsciiTable(table_data).table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cherry-picked tokens whose occurence count will act as features\n",
    "tokens_to_check = ['i', \n",
    "                   'you', \n",
    "                   'me', \n",
    "                   'we', \n",
    "                   'my', \n",
    "                   'this', \n",
    "                   'that', \n",
    "                   'it', \n",
    "                   'like',\n",
    "                   '--', \n",
    "                   '-', \n",
    "                   \"''\", \n",
    "                   '%', \n",
    "                   'said',\n",
    "                   'told',\n",
    "                   '?', \n",
    "                   '!', \n",
    "                   '’', \n",
    "                   ':']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building feature set: 100%|██████████| 20000/20000 [09:52<00:00, 33.75it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "feature_set = []\n",
    "for i in tqdm(range(reduced_article_db.shape[0]), desc='Building feature set'):\n",
    "    article = reduced_article_db.iloc[i]\n",
    "    article_text = article['content']\n",
    "    \n",
    "    tokens = nltk.word_tokenize(article_text.lower())\n",
    "    sentences = nltk.sent_tokenize(article_text.lower())\n",
    "    \n",
    "    article_features = {}\n",
    "    article_features['article_length'] = len(article_text)\n",
    "    # Average token length\n",
    "    article_features['token_length'] = sum(len(t) for t in tokens) / len(tokens)\n",
    "    # Spelling errors per token\n",
    "    article_features['spelling_errors'] = get_spelling_errors(article_text) / len(tokens)\n",
    "    \n",
    "    # Get token count in the article\n",
    "    token_counts = get_token_count(tokens)\n",
    "\n",
    "    # Token count features\n",
    "    for token in tokens_to_check:\n",
    "        article_features[token] = token_counts.get(token, 0)\n",
    "      \n",
    "    # Sentiment features, averaged per sentence\n",
    "    sentiments = {'pos': 0, 'neg': 0, 'neu': 0, 'comp': 0}\n",
    "    for sentence in sentences:\n",
    "        sentiment = sid.polarity_scores(sentence)\n",
    "        sentiments['pos'] += sentiment['pos']\n",
    "        sentiments['neg'] += sentiment['neg']\n",
    "        sentiments['neu'] += sentiment['neu']\n",
    "        sentiments['comp'] += sentiment['compound']\n",
    "    \n",
    "    article_features['sent_pos'] = sentiments['pos'] / len(sentences)\n",
    "    article_features['sent_neg'] = sentiments['neg'] / len(sentences)\n",
    "    article_features['sent_neu'] = sentiments['neu'] / len(sentences)\n",
    "    article_features['sent_comp'] = sentiments['comp'] / len(sentences)\n",
    "    \n",
    "    # Target value\n",
    "    article_features['blog'] = article['blog']\n",
    "    \n",
    "    feature_set.append(article_features)\n",
    "    \n",
    "dataset = pd.DataFrame(feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "dataset.to_pickle('%s/dataset' % PATH_TO_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
